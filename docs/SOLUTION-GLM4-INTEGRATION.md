# Soluci√≥n: Integraci√≥n GLM-4 con ralph-tui

**Versi√≥n:** 1.1
**Fecha:** 2026-01-17
**Estado:** ‚ö†Ô∏è INVESTIGANDO

---

## üö® Problema Detectado

### Situaci√≥n Actual

| Componente | Estado | Detalle |
|-----------|--------|---------|
| **Configuraci√≥n GLM-4** | ‚úÖ OK | Archivo `~/.config/opencode/settings.json` creado correctamente |
| **Script Wrapper** | ‚úÖ OK | Script `scripts/opencode-glm4.sh` creado y es ejecutable |
| **Prompt US-003** | ‚úÖ OK | Archivo `prompts/US-003.txt` creado con contenido completo |
| **Ejecuci√≥n** | ‚ùå FALLIDA | La opci√≥n `--stream` no es reconocida por opencode |

### Errores Recurrentes

```
error: unknown option '--stream'
error: unknown option '--max-tokens'
```

**Causa Ra√≠z:**

La versi√≥n de opencode integrada con ralph-tui (v2.1.9) no soporta las opciones de l√≠nea de comandos de opencode CLI est√°ndar.

---

## üîç Diagn√≥stico Detallado

### Integraci√≥n Ralph-TUI x Opencode

ralph-tui integra opencode como un **plugin** especial. Esto puede causar:

1. **Argumentos enmascarados** - ralph-tui puede pasar argumentos que opencode no espera
2. **Opciones no soportadas** - Si la versi√≥n de opencode es antigua o limitada, ciertos argumentos no funcionan
3. **Configuraci√≥n de sesi√≥n** - El servicio `zai-coding-plan` puede interferir con el manejo de sesiones

### Verificaci√≥n Necesaria

```bash
# Verificar versi√≥n de opencode
claude --version

# Verificar opciones disponibles
claude --help | grep -E "stream|token|prompt|message"
```

---

## üí° Soluciones Propuestas

### Opci√≥n 1: Uso Directo de Opencode CLI (RECOMENDADA)

**Ventajas:**
- ‚úÖ Sin dependencias de scripts
- ‚úÖ Control total de los argumentos
- ‚úÖ Compatibilidad con todas las versiones de opencode
- ‚úÖ M√°s simple y mantenible

**Uso:**

```bash
# Ejecutar prompt directamente
claude --model glm-4 --prompt "$(cat prompts/US-003.txt)"

# Con l√≠mite de tokens
claude --model glm-4 --prompt "$(cat prompts/US-003.txt)" --max-tokens 10000

# Sin l√≠mite (usar valor del modelo)
claude --model glm-4 --prompt "$(cat prompts/US-003.txt)"
```

---

### Opci√≥n 2: Usar Script sin Opciones Problem√°ticas

**Simplificaci√≥n del script:**

```bash
# Solo usar argumentos b√°sicos
claude --model "$MODEL" -p "$(cat "$PROMPT_FILE")"

# Si es necesario, usar variables de entorno
export OPCODE_MODEL=glm-4

# En el script:
claude --model "$OPCODE_MODEL" -p "$(cat "$PROMPT_FILE")"
```

---

### Opci√≥n 3: Revertir a Ralph-TUI Nativo

Si el problema es espec√≠fico de la integraci√≥n con ralph-tui, revertir al uso de ralph-tui con el servicio por defecto (que usa el modelo configurado en ralph-tui):

```bash
# Verificar configuraci√≥n de ralph-tui
cat .ralph-tui/.ralphrc

# Usar modelo configurado en .ralphrc
ralph-tui run --prd ./prd.json
```

---

## üéØ Recomendaci√≥n Inmediata

**USAR OPCI√ìN 1: Uso Directo de Opencode CLI**

```bash
cd /Users/carlosjperez/Documents/GitHub/SOLARIA-DFO
claude --model glm-4 --prompt "$(cat prompts/US-003.txt)" --stream
```

Esto permite:
- Ejecutar la tarea US-003 directamente
- Usar GLM-4 para generar el c√≥digo del endpoint `/stats`
- No depender del script wrapper ni de ralph-tui

---

## üìä Estado Actual

| Componente | Estado |
|-----------|--------|
| Configuraci√≥n GLM-4 | ‚úÖ Completado |
| Script Wrapper | ‚úÖ Creado |
| Prompt US-003 | ‚úÖ Creado |
| Documentaci√≥n | ‚úÖ Actualizada |
| Ejecuci√≥n | ‚ö†Ô∏è Pendiente de soluci√≥n |

---

## üîÑ Pr√≥ximos Pasos Sugeridos

1. Probar Opci√≥n 1 (uso directo de opencode CLI)
2. Si funciona, crear scripts adicionales para US-004 a US-006
3. Documentar el flujo de trabajo establecido
4. Crear template de prompt reutilizable para las siguientes tareas

---

## üìù Notas Importantes

- **Script wrapper a√∫n √∫til**: Aunque no funciona con `--stream`, el script puede ser reutilizado cuando se resuelva el problema de integraci√≥n
- **Configuraci√≥n GLM-4 mantiene**: La configuraci√≥n en `~/.config/opencode/settings.json` sigue siendo v√°lida para uso directo de opencode
- **Prompt US-003 est√° listo**: Contiene toda la informaci√≥n necesaria para implementar el endpoint `/stats`

---

**Versi√≥n:** 1.1 (Actualizada)
**Pr√≥xima revisi√≥n:** Cuando se resuelva el problema de integraci√≥n
